from __future__ import annotations
from dataclasses import dataclass
from typing import Iterable, Sequence
from urllib.parse import urlparse
from difflib import SequenceMatcher
from deep_research_agent.core.state import ResearchArtifact

@dataclass(slots=True)
class EvidenceRecord:
    """Structured reference to a supporting source."""

    url: str
    score: float
    snippet: str
    domain: str | None = None
    title: str | None = None

    def to_dict(self) -> dict[str, object]:
        return {
            "url": self.url,
            "score": round(self.score, 3),
            "snippet": self.snippet,
            "domain": self.domain,
            "title": self.title,
        }


@dataclass(slots=True)
class ValidationResult:
    """Represents the validation status of a single fact."""

    fact: str
    normalized_fact: str
    support_mentions: int
    unique_domains: int
    confidence: str
    score: float
    evidence: list[EvidenceRecord]
    notes: list[str]

    def to_dict(self) -> dict[str, object]:
        return {
            "fact": self.fact,
            "normalized_fact": self.normalized_fact,
            "support_mentions": self.support_mentions,
            "unique_domains": self.unique_domains,
            "confidence": self.confidence,
            "score": round(self.score, 3),
            "evidence": [record.to_dict() for record in self.evidence],
            "notes": self.notes,
        }


DEFAULT_MIN_MATCH_RATIO = 0.58
DEFAULT_MIN_TOKEN_OVERLAP = 3


def normalize_fact_text(text: str) -> str:
    """Normalize a fact string for comparison purposes."""
    normalized = " ".join(text.strip().lower().split())
    return normalized.rstrip(".") if normalized else normalized


def _tokenize(text: str) -> list[str]:
    return [token for token in text.replace(",", " ").split() if token]


def _trim_snippet(snippet: str, max_length: int = 420) -> str:
    snippet = snippet.strip()
    if len(snippet) <= max_length:
        return snippet
    return snippet[: max_length - 3].rstrip() + "..."


def _match_score(fact: str, snippet: str) -> float:
    if not fact or not snippet:
        return 0.0
    matcher = SequenceMatcher(None, fact, snippet.lower())
    return matcher.quick_ratio()


def _token_overlap(fact_tokens: Sequence[str], snippet_tokens: Sequence[str]) -> int:
    snippet_set = set(snippet_tokens)
    return sum(1 for token in fact_tokens if token in snippet_set)


def _extract_domain(artifact: ResearchArtifact) -> str | None:
    if artifact.metadata and isinstance(artifact.metadata, dict):
        domain = artifact.metadata.get("domain")
        if isinstance(domain, str) and domain:
            return domain.lower()
    if artifact.url:
        parsed = urlparse(artifact.url)
        if parsed.netloc:
            return parsed.netloc.lower()
    return None


def _confidence_from_metrics(
    support_mentions: int,
    unique_domains: int,
    max_score: float,
) -> tuple[str, float]:
    weighted_score = min(1.0, max_score) * 0.5
    weighted_score += min(support_mentions / 4, 1.0) * 0.3
    weighted_score += min(unique_domains / 3, 1.0) * 0.2

    if support_mentions >= 3 and unique_domains >= 2 and max_score >= 0.72:
        confidence = "high"
    elif support_mentions >= 2 and max_score >= 0.6:
        confidence = "medium"
    elif support_mentions >= 1:
        confidence = "low"
    else:
        confidence = "none"

    return confidence, min(round(weighted_score, 3), 1.0)


def validate_facts(
    findings: Sequence[ResearchArtifact],
    candidate_facts: Iterable[str],
    *,
    min_match_ratio: float = DEFAULT_MIN_MATCH_RATIO,
    min_token_overlap: int = DEFAULT_MIN_TOKEN_OVERLAP,
    return_all: bool = True,
) -> list[ValidationResult]:
    """Cross-check candidate facts against collected findings.

    Args:
        findings: Research artifacts accumulated by the search node.
        candidate_facts: Facts generated by extraction node or heuristics.
        min_match_ratio: Minimum fuzzy matching score to consider as supporting.
        min_token_overlap: Minimum overlapping tokens between fact and snippet.
        return_all: When False, only return facts with supporting evidence.

    Returns:
        Ordered list of ValidationResult objects.
    """
    results: list[ValidationResult] = []

    for raw_fact in candidate_facts:
        normalized_fact = normalize_fact_text(raw_fact)
        if not normalized_fact:
            continue

        fact_tokens = _tokenize(normalized_fact)
        evidence: list[EvidenceRecord] = []
        seen_domains: set[str] = set()

        for artifact in findings:
            snippet = (artifact.snippet or "").strip()
            if not snippet:
                continue

            snippet_tokens = _tokenize(snippet.lower())
            overlap = _token_overlap(fact_tokens, snippet_tokens)
            match_ratio = _match_score(normalized_fact, snippet)

            if match_ratio < min_match_ratio and overlap < min_token_overlap:
                continue

            domain = _extract_domain(artifact)
            if domain:
                seen_domains.add(domain)

            evidence.append(
                EvidenceRecord(
                    url=artifact.url,
                    score=max(match_ratio, overlap / max(len(fact_tokens), 1)),
                    snippet=_trim_snippet(snippet),
                    domain=domain,
                    title=artifact.title,
                )
            )

        support_mentions = len(evidence)
        unique_domains = len(seen_domains)
        max_score = max((record.score for record in evidence), default=0.0)
        confidence, aggregate_score = _confidence_from_metrics(
            support_mentions, unique_domains, max_score
        )

        notes: list[str] = []
        if support_mentions == 0:
            notes.append("No supporting sources identified; manual verification required.")
        elif unique_domains <= 1:
            notes.append("Evidence lacks source diversity.")
        if max_score < min_match_ratio and support_mentions > 0:
            notes.append("Fuzzy match scores are below the configured threshold.")

        result = ValidationResult(
            fact=raw_fact,
            normalized_fact=normalized_fact,
            support_mentions=support_mentions,
            unique_domains=unique_domains,
            confidence=confidence,
            score=aggregate_score,
            evidence=evidence,
            notes=notes,
        )

        if return_all or support_mentions > 0:
            results.append(result)

    return sorted(results, key=lambda item: item.score, reverse=True)


__all__ = [
    "EvidenceRecord",
    "ValidationResult",
    "validate_facts",
    "normalize_fact_text",
]
